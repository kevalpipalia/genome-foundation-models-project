{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979a5383-d8df-4170-b790-a9149168e4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq = \"TTCCACAAATACCTAAGTATTCTTTAATAATGGTGGTTTTTTTTTTTTTTTGCATCTATGAAGTTTTTTCAAATTCTTTTTAAGTGACAAAACTTGTACATGTGTATCGCTCAATATTTCTAGTCGACAGCACTGCTTTCGAGAATGTAAACCGTGCACTCCCAGGAAAATGCAGACACAGCACGCCTCTTTGGGACCGCGGTTTATACTTTCGAAGTGCTCGGAGCCCTTCCTCCAGACCGTTCTCCCACACCCCGCTCCAGGGTCTCTCCCGGAGTTACAAGCCTCGCTGTAGGCCCCGGGAACCCAACGCGGTGTCAGAGAAGTGGGGTCCCCTACGAGGGACCAGGAGCTCCGGGCGGGCAGCAGCTGCGGAAGAGCCGCGCGAGGCTTCCCAGAACCCGGCAGGGGCGGGAAGACGCAGGAGTGGGGAGGCGGAACCGGGACCCCGCAGAGCCCGGGTCCCTGCGCCCCACAAGCCTTGGCTTCCCTGCTAGGGCCGGGCAAGGCCGGGTGCAGGGCGCGGCTCCAGGGAGGAAGCTCCGGGGCGAGCCCAAGACGCCTCCCGGGCGGTCGGGGCCCAGCGGCGGCGTTCGCAGTGGAGCCGGGCACCGGGCAGCGGCCGCGGAACACCAGCTTGGCGCAGGCTTCTCGGTCAGGAACGGTCCCGGGCCTCCCGCCCGCCTCCCTCCAGCCCCTCCGGGTCCCCTACTTCGCCCCGCCAGGCCCCCACGACCCTACTTCCCGCGGCCCCGGACGCCTCCTCACCTGCGAGCCGCCCTCCCGGAAGCTCCCGCCGCCGCTTCCGCTCTGCCGGAGCCGCTGGGTCCTAGCCCCGCCGCCCCCAGTCCGCCCGCGCCTCCGGGTCCTAACGCCGCCGCTCGCCCTCCACTGCGCCCTCCCCGAGCGCGGCTCCAGGACCCCGTCGACCCGGAGCGCTGTCCTGTCGGGCCGAGTCGCGGGCCTGGGCACGGAACTCACGCTCACTCCGAGCTCCCGACGTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCCTACCCGTGCTTTCTGCTCTGCAGACCCTCTTCCTAGACCTCCGTCCTTTGTCCCATCGCTGCCTTCCCCTCAAGCTCAGGGCCAAGCTGTCCGCCAACCTCGGCTCCTCCGGGCAGCCCTCGCCCGGGGTGCGCCCCGGGGCAGGacccccagcccacgcccagggcccgcccctgccctccagccctacgccTTGACCCGCTTTCCTGCGTCTCTCAGCCTACCTGACCTTGTCTTTACCTCTGTGGGCAGCTCCCTTGTGATCTGCTTAGTTCCCACCCCCCTTTAAGAATTCAATAGAGaagccagacgcaaaactacagatatcgtatgagtccagttttgtgaagtgcctagaatagtcaaaattcacagagacagaagcagtggtcgccaggaatggggaagcaaggcggagttgggcagctcgtgttcaatgggtagagtttcaggctggggtgatggaagggtgctggaaatgagtggtagtgatggcggcacaacagtgtgaatctacttaatcccactgaactgtatgctgaaaaatggtttagacggtgaattttaggttatgtatgttttaccacaatttttaaaaaGCTAGTGAAAAGCTGGTAAAAAGAAAGAAAAGAGGCTTTTTTAAAAAGTTAAATATATAAAAAGAGCATCATCAGTCCAAAGTCCAGCAGTTGTCCCTCCTGGAATCCGTTGGCTTGCCTCCGGCATTTTTGGCCCTTGCCTTTtagggttgccagattaaaagacaggatgcccagctagtttgaattttagataaacaacgaataatttcgtagcataaatatgtcccaagcttagtttgggacatacttatgctaaaaaacattattggttgtttatctgagattcagaattaagcattttatattttatttgctgcctctggccaccctaCTCTCTTCCTAACACTCTCTC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a4a08a-ab82-48b5-ab49-e25bb3a2eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seq.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4529b1a-b7c2-47f8-b28f-49b5fe74cebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 337, 1280)\n"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "\n",
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=\"500M_human_ref\",\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    max_positions=337,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)\n",
    "\n",
    "# Get data and tokenize it\n",
    "sequences = [seq]\n",
    "tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "\n",
    "# Initialize random key\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Infer\n",
    "outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "\n",
    "# Get embeddings at layer 20\n",
    "print(outs[\"embeddings_20\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd735e5-11d2-4858-bf21-05f906289333",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outs[\"embeddings_20\"][:, 1:, :]  # removing CLS token\n",
    "padding_mask = jnp.expand_dims(tokens[:, 1:] != tokenizer.pad_token_id, axis=-1)\n",
    "masked_embeddings = embeddings * padding_mask  # multiply by 0 pad tokens embeddings\n",
    "sequences_lengths = jnp.sum(padding_mask, axis=1)\n",
    "mean_embeddings = jnp.sum(masked_embeddings, axis=1) / sequences_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94510314-e67a-4b3a-975b-1b1651ae5b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1280)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf9e8b9-ddc1-455f-ae59-c69a902076d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "\n",
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=\"500M_human_ref\",\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    max_positions=337,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_NT_embeddings(sequence):\n",
    "    # Get data and tokenize it\n",
    "    sequences = [seq]\n",
    "    tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "    tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "    tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "    \n",
    "    # Initialize random key\n",
    "    random_key = jax.random.PRNGKey(0)\n",
    "    \n",
    "    # Infer\n",
    "    outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "\n",
    "    embeddings = outs[\"embeddings_20\"][:, 1:, :]  # removing CLS token\n",
    "    padding_mask = jnp.expand_dims(tokens[:, 1:] != tokenizer.pad_token_id, axis=-1)\n",
    "    masked_embeddings = embeddings * padding_mask  # multiply by 0 pad tokens embeddings\n",
    "    sequences_lengths = jnp.sum(padding_mask, axis=1)\n",
    "    mean_embeddings = jnp.sum(masked_embeddings, axis=1) / sequences_lengths\n",
    "    return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205e1698-3d17-404f-aedd-df7e8ef99d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "# Replace 'your_file.bed.gz' with your file path\n",
    "file_path = 'data/Whole_Blood.v8.normalized_expression.bed.gz'\n",
    "\n",
    "# Use gzip.open to read the compressed file\n",
    "with gzip.open(file_path, 'rt') as file:\n",
    "    # Read the file into a pandas DataFrame\n",
    "    # BED format is typically tab-delimited; no header row\n",
    "    df = pd.read_csv(file, sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36221809-e3f9-4cc6-bc98-7b0acbb0f515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ec666c-1fc3-4c78-bc8f-d15e759efd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa24006-7167-4aa2-aa97-4433dbdc3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame(columns=['sequence', 'gtex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0899c9a6-47ff-4cab-a52d-c6e0a5e7f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>gtex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sequence, gtex]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b346cd-967f-4e99-b71b-269539bf2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.empty([640, 1280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a19c23-9056-47fd-878c-cde5447b42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = np.empty([640,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46235e30-7a3e-48ee-897e-607ea80110bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 640 is out of bounds for axis 0 with size 640",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m sequence \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     13\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_NT_embeddings(sequence)\n\u001b[0;32m---> 14\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embeddings[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m target_values[index] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     16\u001b[0m index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 640 is out of bounds for axis 0 with size 640"
     ]
    }
   ],
   "source": [
    "# Specify the path to your FASTA file\n",
    "fasta_file = \"data/personalized_sequences/1A3MV_sequences_windowed.fasta\"\n",
    "index = 0\n",
    "# Read the FASTA file\n",
    "for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    seq_id = seq_record.id\n",
    "    sequence = seq_record.seq\n",
    "    seq_start, seq_end = seq_id.split(\":\")[-1].split(\"-\")\n",
    "    snp_id = int(seq_start) + 1000\n",
    "    value = df[df['start']==snp_id]['GTEX-1A3MV'].values[0]\n",
    "    # training_df = training_df.append({'sequence': sequence, 'gtex': value})\n",
    "    sequence = sequence.upper()\n",
    "    embeddings = get_NT_embeddings(sequence)\n",
    "    input_data[index] = np.array(embeddings[0])\n",
    "    target_values[index] = value\n",
    "    index+=1\n",
    "    print(index)\n",
    "    # training_df['sequence'] = embeddings\n",
    "    # training_df['gtex'] = value\n",
    "    # print(\"ID: %s\" % seq_record.id)\n",
    "    # print(\"Sequence length: %i\" % len(seq_record))\n",
    "    # print(\"Sequence: %s\" % seq_record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42724014-43c4-4ba2-a2fe-72bfd120566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42531732,  0.63464391,  1.5967803 , ...,  1.23872888,\n",
       "         0.16475847, -0.6549775 ],\n",
       "       [-0.42531732,  0.63464391,  1.5967803 , ...,  1.23872888,\n",
       "         0.16475847, -0.6549775 ],\n",
       "       [-0.42531732,  0.63464391,  1.5967803 , ...,  1.23872888,\n",
       "         0.16475847, -0.6549775 ],\n",
       "       ...,\n",
       "       [-0.42531732,  0.63464391,  1.5967803 , ...,  1.23872888,\n",
       "         0.16475847, -0.6549775 ],\n",
       "       [-0.42531732,  0.63464391,  1.5967803 , ...,  1.23872888,\n",
       "         0.16475847, -0.6549775 ],\n",
       "       [-0.42531732,  0.63464391,  1.5967803 , ...,  1.23872888,\n",
       "         0.16475847, -0.6549775 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abe9be29-2529-4b47-ac05-97b4cccfcf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.97583769e-01, -3.54962286e-02,  1.93588933e-01, -3.21096703e-01,\n",
       "       -3.21096703e-01, -9.54038745e-02, -4.25268664e-01,  9.08457869e-01,\n",
       "        1.24100185e+00,  5.41935946e-02,  2.16479492e-01, -2.42841806e-02,\n",
       "        6.31718572e-01,  1.20162669e+00,  2.70357483e-01,  5.12862238e-01,\n",
       "       -2.80211212e-02,  3.36870393e-01,  8.79010896e-02, -6.63974849e-01,\n",
       "        1.02784266e+00, -9.91572554e-02,  5.73481197e-01, -5.51600554e-01,\n",
       "       -1.17141825e+00, -2.31802384e-01, -7.35562454e-01, -1.02912034e-01,\n",
       "        2.05475792e-02, -9.02828374e-01, -1.84132620e+00, -7.55290803e-01,\n",
       "       -7.95660299e-01,  2.62616089e-01, -5.95639927e-01,  1.88364863e+00,\n",
       "       -1.04059707e+00, -7.65265835e-01,  8.58779831e-01,  4.13032623e-01,\n",
       "        1.89784005e-01,  4.54072768e-01,  8.86108863e-01, -1.55656200e-01,\n",
       "        2.97583769e-01, -3.92345001e-02,  4.29361443e-01,  5.17127634e-01,\n",
       "        1.10425999e-01,  1.14944489e+00,  7.16116327e-01, -1.02152775e+00,\n",
       "        3.25032489e-01,  9.84462366e-01,  7.29099270e-02,  7.65265835e-01,\n",
       "        4.41683390e-01,  1.71449338e+00,  2.80211212e-02, -6.04585347e-01,\n",
       "        3.36870393e-01, -4.04909596e-01, -9.42859822e-01,  7.70281989e-01,\n",
       "        1.86207056e+00, -1.15670733e+00,  6.13579409e-01,  7.25805082e-01,\n",
       "        3.36870393e-01, -4.79064565e-01, -1.93588933e-01, -2.70357483e-01,\n",
       "        1.24911110e+00, -3.76684147e-01,  3.05402507e-01, -1.97396666e-01,\n",
       "        8.21561729e-01,  5.42922290e-01, -7.66560503e-02, -3.17165885e-01,\n",
       "       -5.47256270e-01,  5.77890128e-01,  4.87461323e-01,  1.50959209e+00,\n",
       "        1.50959209e+00,  1.38218452e+00,  9.96665348e-01,  1.12801294e+00,\n",
       "        5.38598483e-01,  1.68112646e-02,  1.48664426e+00,  5.42922290e-01,\n",
       "        3.56706585e-01,  2.62616089e-01, -1.10364619e+00,  3.44788535e-01,\n",
       "       -1.02912034e-01,  6.87443523e-01, -4.08967737e-01,  2.82000185e-01,\n",
       "        1.74591161e-01,  9.78415526e-01,  4.70701449e-01, -8.97227347e-01,\n",
       "        1.55803464e+00, -1.97396666e-01,  1.57073416e+00,  1.22502363e+00,\n",
       "        1.17141825e+00,  7.45390364e-01,  4.04909596e-01,  8.26808251e-01,\n",
       "       -1.02784266e+00,  6.27164492e-01, -5.21402460e-01, -1.48098193e-01,\n",
       "       -9.37049232e-01,  7.85448319e-01, -5.41935946e-02, -6.22623383e-01,\n",
       "       -2.82000185e-01, -4.79064565e-01, -3.01490834e-01, -5.91185073e-01,\n",
       "        1.86783140e-03,  3.17584530e-02, -8.79010896e-02,  6.09076219e-01,\n",
       "        2.35641485e-01,  1.23297339e+00, -8.48026435e-01,  7.11297099e-01,\n",
       "        8.16337723e-01,  2.80211212e-02, -1.44322386e-01,  4.04909596e-01,\n",
       "       -1.24911110e+00, -3.92774768e-01, -3.17584530e-02,  6.22623383e-01,\n",
       "        9.60485888e-01, -4.74878855e-01, -1.24100185e+00,  7.65265835e-01,\n",
       "        5.17127634e-01, -9.60485888e-01, -8.91654327e-01,  9.02828374e-01,\n",
       "        5.25686835e-01, -1.29239167e-01, -3.28973316e-01,  8.16337723e-01,\n",
       "        1.59438513e-01,  2.39484062e-01, -3.48755696e-01, -1.04703818e+00,\n",
       "       -1.78385459e-01,  2.51033285e-01,  3.21096703e-01,  1.27394597e+00,\n",
       "       -4.21182995e-01, -9.16518371e-02, -4.13032623e-01, -9.91572554e-02,\n",
       "        4.17104346e-01,  6.40866323e-01,  5.77890128e-01,  3.25032489e-01,\n",
       "       -2.08837289e-01,  6.40866323e-01,  1.34434282e+00, -4.83258685e-01,\n",
       "       -2.74234236e-01, -5.91185073e-01, -1.36301969e+00, -1.27394597e+00,\n",
       "        7.80372948e-01, -8.16337723e-01,  2.21404573e+00,  6.54689542e-01,\n",
       "        8.75098856e-01, -4.29361443e-01, -2.24134360e-01,  6.27164492e-01,\n",
       "       -3.60690470e-01,  8.00797507e-01, -2.35641485e-01, -1.78385459e-01,\n",
       "       -4.41683390e-01, -1.19397225e+00, -4.54072768e-01, -9.60485888e-01,\n",
       "        1.18638714e+00,  1.10425999e-01, -9.14116301e-01,  6.04585347e-01,\n",
       "        8.05955937e-01,  9.60485888e-01,  1.38218452e+00,  9.96665348e-01,\n",
       "        1.02152775e+00,  5.86741920e-01, -6.09076219e-01, -8.91654327e-01,\n",
       "        3.21096703e-01, -5.73481197e-01, -1.36776390e+00,  4.29733200e-02,\n",
       "        7.01707811e-01, -7.60268863e-01, -1.10021844e+00,  2.51033285e-01,\n",
       "        4.70701449e-01, -1.70799376e-01, -1.00282284e+00,  9.91572554e-02,\n",
       "        9.48702421e-01, -8.11135902e-01,  6.73317760e-01, -8.16337723e-01,\n",
       "       -1.74792566e+00,  6.78011297e-01,  7.65265835e-01,  1.78385459e-01,\n",
       "        2.05020784e-01,  9.48702421e-01,  2.74234236e-01,  1.44322386e-01,\n",
       "        6.50068019e-01, -3.44788535e-01, -3.17584530e-02, -3.60690470e-01,\n",
       "        6.04585347e-01, -4.70701449e-01, -3.28973316e-01,  8.37370218e-01,\n",
       "       -2.70357483e-01,  9.84462366e-01,  3.84717006e-01,  8.53390798e-01,\n",
       "        1.21708783e-01,  1.37253909e+00, -1.25730334e+00, -4.29733200e-02,\n",
       "       -9.96665348e-01,  3.88742732e-01,  1.30751847e-02, -1.11400811e+00,\n",
       "       -1.30751847e-02, -6.54206962e-02, -8.69633462e-01,  7.90544004e-01,\n",
       "        4.45805560e-01,  2.80211212e-02,  8.21561729e-01, -1.31717661e+00,\n",
       "       -1.11400811e+00, -1.22502363e+00, -1.59692171e+00,  9.33928731e-03,\n",
       "       -3.54962286e-02, -8.79010896e-02, -7.95660299e-01,  4.17104346e-01,\n",
       "        4.25268664e-01, -2.97583769e-01,  5.95639927e-01,  6.63974849e-01,\n",
       "        3.76684147e-01, -3.54962286e-02, -9.02828374e-01, -5.64696554e-01,\n",
       "        1.51876112e-01, -2.54890402e-01, -5.64696554e-01, -7.30675072e-01,\n",
       "        1.38218452e+00,  7.85448319e-01, -2.05475792e-02,  9.19804155e-01,\n",
       "       -1.14944489e+00,  1.34434282e+00,  9.08457869e-01,  3.28973316e-01,\n",
       "       -3.44788535e-01, -6.13579409e-01,  6.63974849e-01, -6.36285792e-01,\n",
       "       -1.37253909e+00,  1.30833397e+00,  8.91654327e-01,  4.91672588e-01,\n",
       "        1.08663486e+00,  1.51876112e-01, -1.00282284e+00,  7.70281989e-01,\n",
       "        4.49935320e-01, -1.86783140e-03,  9.16518371e-02, -4.29733200e-02,\n",
       "        7.95660299e-01, -7.30675072e-01, -7.23376534e-01,  1.12098304e+00,\n",
       "       -2.47179898e-01, -5.47256270e-01,  1.13509902e+00, -6.78011297e-01,\n",
       "       -1.10708696e+00, -9.60485888e-01, -7.40467469e-01, -4.00858113e-01,\n",
       "       -1.93588933e-01, -1.00282284e+00,  1.18638714e+00, -8.00797507e-01,\n",
       "        2.01207263e-01, -2.24134360e-01,  1.10425999e-01,  1.25473085e-01,\n",
       "       -1.05352304e+00,  2.80211212e-02,  1.14944489e+00, -1.58369218e+00,\n",
       "        5.55955274e-01,  5.04528147e-02, -1.58369218e+00,  8.00797507e-01,\n",
       "       -5.04359259e-01,  1.19397225e+00,  1.51876112e-01, -9.72404250e-01,\n",
       "        1.74792566e+00, -1.82134522e+00, -1.30833397e+00,  8.86108863e-01,\n",
       "        9.60485888e-01, -4.74878855e-01, -7.16116327e-01, -5.69083386e-01,\n",
       "       -1.25730334e+00,  6.92182619e-01, -7.06494334e-01,  8.86108863e-01,\n",
       "        1.66767616e+00, -2.43330179e-01,  5.25686835e-01,  1.24100185e+00,\n",
       "       -1.93588933e-01,  6.18095076e-01, -7.20952245e-01, -1.47545812e+00,\n",
       "        1.59692171e+00, -1.60364264e+00, -5.60352025e-03, -2.70357483e-01,\n",
       "       -2.78115114e-01,  1.90614117e+00,  1.10708696e+00, -7.29099270e-02,\n",
       "       -8.11135902e-01, -6.54206962e-02, -1.02152775e+00,  4.00858113e-01,\n",
       "       -9.72404250e-01, -3.84717006e-01,  4.25268664e-01, -1.50959209e+00,\n",
       "        8.11135902e-01, -1.65285363e+00, -5.60320562e-01,  1.34434282e+00,\n",
       "        3.32919259e-01,  5.69083386e-01,  2.58751315e-01,  1.58369218e+00,\n",
       "        3.17165885e-01,  4.70701449e-01,  6.68639009e-01,  5.25686835e-01,\n",
       "       -8.79010896e-02,  5.82310320e-01,  5.91185073e-01,  1.71449338e+00,\n",
       "        9.37049232e-01,  6.09076219e-01, -4.67127407e-02,  1.00282284e+00,\n",
       "       -1.22502363e+00,  9.02828374e-01,  3.96813200e-01,  4.66532241e-01,\n",
       "        1.80206600e+00,  8.75098856e-01,  4.25268664e-01, -3.44788535e-01,\n",
       "        4.29733200e-02, -1.44296157e+00,  8.86108863e-01,  1.33007082e-01,\n",
       "        1.08663486e+00, -6.50068019e-01,  2.74234236e-01,  3.92345001e-02,\n",
       "       -1.52137039e+00, -9.91572554e-02, -2.82000185e-01, -3.68675520e-01,\n",
       "        6.18095076e-01,  1.70799376e-01,  7.16116327e-01, -6.16774826e-02,\n",
       "        2.70357483e-01,  8.05955937e-01, -7.65265835e-01,  1.25473085e-01,\n",
       "        5.60320562e-01,  4.25268664e-01, -4.33461427e-01, -5.04528147e-02,\n",
       "        1.48664426e+00, -5.17127634e-01,  3.80697506e-01, -1.53336361e+00,\n",
       "       -1.41192117e+00, -2.47179898e-01, -9.96665348e-01, -6.04585347e-01,\n",
       "        4.62371127e-01, -4.04909596e-01, -1.67010044e-01, -6.50068019e-01,\n",
       "       -1.29094915e+00,  3.92774768e-01, -2.89783180e-01, -3.25032489e-01,\n",
       "        5.95639927e-01,  1.95425861e+00, -1.01525331e+00,  1.02912034e-01,\n",
       "        2.93681241e-01,  1.14185294e-01,  5.79351329e-02,  1.11400811e+00,\n",
       "        5.21402460e-01, -5.08606152e-01, -1.73096770e+00, -3.05402507e-01,\n",
       "        4.74878855e-01, -1.78343421e+00,  3.52728353e-01,  6.91648267e-02,\n",
       "       -2.82000185e-01,  2.43330179e-01,  9.31270109e-01, -6.31718572e-01,\n",
       "        1.00901858e+00, -4.70701449e-01, -2.43330179e-01, -1.84132620e+00,\n",
       "       -8.86108863e-01,  6.09076219e-01, -2.20305313e-01,  3.60690470e-01,\n",
       "       -2.97583769e-01,  4.87461323e-01,  3.76684147e-01, -6.59325090e-01,\n",
       "       -1.17141825e+00,  5.69083386e-01, -1.89784005e-01,  9.54038745e-02,\n",
       "       -3.32919259e-01,  5.60320562e-01, -3.84717006e-01,  1.59692171e+00,\n",
       "        1.53336361e+00,  1.49801957e+00, -6.63974849e-01, -5.73481197e-01,\n",
       "        9.08457869e-01,  1.17946204e-01,  7.01707811e-01,  7.70281989e-01,\n",
       "        1.37253909e+00, -7.75317600e-01,  6.87443523e-01,  3.09318859e-01,\n",
       "        9.84462366e-01,  4.79064565e-01,  1.69847166e+00,  8.58779831e-01,\n",
       "       -1.51876112e-01, -1.29094915e+00, -5.34284723e-01,  6.31718572e-01,\n",
       "        7.25805082e-01,  3.92345001e-02,  4.13032623e-01,  9.78415526e-01,\n",
       "        8.97227347e-01, -8.04032496e-02, -1.82182326e-01,  6.92182619e-01,\n",
       "        3.05402507e-01,  2.36801614e+00,  2.17265545e+00,  1.06668264e-01,\n",
       "        6.82719818e-01,  1.82182326e-01, -4.66532241e-01,  6.68639009e-01,\n",
       "        5.51600554e-01, -5.82310320e-01, -6.36285792e-01, -1.58369218e+00,\n",
       "       -1.49801957e+00, -6.73317760e-01,  1.30833397e+00, -3.28973316e-01,\n",
       "       -4.66532241e-01, -1.38218452e+00, -1.89784005e-01, -1.70799376e-01,\n",
       "       -8.48026435e-01,  1.93588933e-01,  1.02912034e-01, -8.21561729e-01,\n",
       "       -1.37253909e+00,  2.05020784e-01,  1.68112646e-02,  3.44788535e-01,\n",
       "        1.12098304e+00,  2.89783180e-01, -4.83258685e-01,  9.19804155e-01,\n",
       "        9.90545419e-01,  9.37049232e-01, -6.73317760e-01,  1.37253909e+00,\n",
       "       -7.90544004e-01,  4.49935320e-01,  2.51033285e-01, -9.60485888e-01,\n",
       "       -1.38218452e+00, -5.08606152e-01,  5.51600554e-01, -6.96937311e-01,\n",
       "       -2.97583769e-01,  1.89784005e-01,  8.97227347e-01, -2.39484062e-01,\n",
       "        4.17104346e-01, -1.55656200e-01, -1.74792566e+00, -1.17141825e+00,\n",
       "       -1.62425261e+00,  2.43330179e-01,  3.28973316e-01, -3.80697506e-01,\n",
       "        3.68675520e-01,  4.87461323e-01, -6.27164492e-01, -7.45390364e-01,\n",
       "       -1.24100185e+00, -9.19804155e-01,  4.45805560e-01,  7.70281989e-01,\n",
       "        7.30675072e-01,  3.54962286e-02,  5.77890128e-01, -1.29239167e-01,\n",
       "       -8.32077632e-01,  1.68287442e+00,  1.01525331e+00, -7.29099270e-02,\n",
       "        8.32077632e-01,  2.82000185e-01, -1.17141825e+00, -7.95660299e-01,\n",
       "        1.55656200e-01,  8.91654327e-01, -1.19397225e+00,  1.14185294e-01,\n",
       "        9.91572554e-02,  1.09340143e+00,  1.12098304e+00, -5.77890128e-01,\n",
       "       -9.33928731e-03,  1.40548636e-01,  4.17104346e-01, -1.93588933e-01,\n",
       "        7.50331390e-01, -1.82182326e-01,  1.40187090e+00,  1.70799376e-01,\n",
       "        1.12098304e+00,  7.65265835e-01,  1.67010044e-01, -1.82182326e-01,\n",
       "        2.16479492e-01, -4.21182995e-01, -2.42841806e-02,  9.54038745e-02,\n",
       "        4.66532241e-01, -1.27394597e+00, -1.12098304e+00, -4.83258685e-01,\n",
       "        7.66560503e-02,  6.31718572e-01,  7.40467469e-01, -5.17127634e-01,\n",
       "       -4.54072768e-01, -4.13032623e-01,  1.21708783e-01,  6.78011297e-01,\n",
       "        1.12098304e+00,  8.11135902e-01, -2.12656838e-01,  7.55290803e-01,\n",
       "        1.17141825e+00, -1.59438513e-01, -7.50331390e-01,  1.78385459e-01,\n",
       "        5.64696554e-01,  6.09076219e-01, -2.78115114e-01,  4.08967737e-01,\n",
       "        1.25730334e+00,  3.13239961e-01,  7.01707811e-01, -2.97583769e-01,\n",
       "        4.41683390e-01,  3.44788535e-01,  6.92182619e-01,  2.35641485e-01,\n",
       "        1.48664426e+00,  1.93588933e-01,  1.93588933e-01, -5.60320562e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d841fedf-dbb5-412c-86c2-9de6d502f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_data.npy', input_data)\n",
    "np.save('target_values.npy', target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f60b263-a47d-4a78-aa5b-5d39ba51e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_data, target_values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d0bab81-8620-41e5-9257-20a8b6031f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (fc1): Linear(in_features=1280, out_features=768, bias=True)\n",
      "  (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=368, bias=True)\n",
      "  (fc4): Linear(in_features=368, out_features=256, bias=True)\n",
      "  (fc5): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc6): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc7): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1280, 768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.fc3 = nn.Linear(512, 368)\n",
    "        self.fc4 = nn.Linear(368, 256)\n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.fc7 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.fc7(x)  # No activation here if it's a regression problem\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "\n",
    "# Display model summary (note: PyTorch does not have an equivalent to model.summary() in Keras)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c18c4dd-3c56-498c-8c9c-a13f3daa34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader for batch processing\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d74f179-9f95-422f-944a-3596d8ebc10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7144\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_test and y_test are your test datasets\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_test(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "test_loss = evaluate_test(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fad6d93b-9c63-4685-9ee1-a237f06620cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [2/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [3/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [4/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [5/100], Training Loss: 0.6067, Validation Loss: 0.7144\n",
      "Epoch [6/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [7/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [8/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [9/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [10/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [11/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [12/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [13/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [14/100], Training Loss: 0.6067, Validation Loss: 0.7144\n",
      "Epoch [15/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [16/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [17/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [18/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [19/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [20/100], Training Loss: 0.6069, Validation Loss: 0.7144\n",
      "Epoch [21/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [22/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [23/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [24/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [25/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [26/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [27/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [28/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [29/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [30/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [31/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [32/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [33/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [34/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [35/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [36/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [37/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [38/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [39/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [40/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [41/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [42/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [43/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [44/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [45/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [46/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [47/100], Training Loss: 0.6069, Validation Loss: 0.7145\n",
      "Epoch [48/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [49/100], Training Loss: 0.6069, Validation Loss: 0.7144\n",
      "Epoch [50/100], Training Loss: 0.6068, Validation Loss: 0.7145\n",
      "Epoch [51/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [52/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [53/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [54/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [55/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [56/100], Training Loss: 0.6068, Validation Loss: 0.7144\n",
      "Epoch [57/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [58/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [59/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [60/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [61/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [62/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [63/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [64/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [65/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [66/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [67/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [68/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [69/100], Training Loss: 0.6068, Validation Loss: 0.7144\n",
      "Epoch [70/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [71/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [72/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [73/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [74/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [75/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [76/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [77/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [78/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [79/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [80/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [81/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [82/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [83/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [84/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [85/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [86/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [87/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [88/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [89/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [90/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [91/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [92/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [93/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [94/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [95/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [96/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [97/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [98/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [99/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [100/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [101/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [102/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [103/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [104/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [105/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [106/100], Training Loss: 0.6068, Validation Loss: 0.7145\n",
      "Epoch [107/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [108/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [109/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [110/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [111/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [112/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [113/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [114/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [115/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [116/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [117/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [118/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [119/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [120/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [121/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [122/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [123/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [124/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [125/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [126/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [127/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [128/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [129/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [130/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [131/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [132/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [133/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [134/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [135/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [136/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [137/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [138/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [139/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [140/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [141/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [142/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [143/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [144/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [145/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [146/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [147/100], Training Loss: 0.6067, Validation Loss: 0.7144\n",
      "Epoch [148/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [149/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [150/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [151/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [152/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [153/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [154/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [155/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [156/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [157/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [158/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [159/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [160/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [161/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [162/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [163/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [164/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [165/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [166/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [167/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [168/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [169/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [170/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [171/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [172/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [173/100], Training Loss: 0.6067, Validation Loss: 0.7144\n",
      "Epoch [174/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [175/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [176/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [177/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [178/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [179/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [180/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [181/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [182/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [183/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [184/100], Training Loss: 0.6069, Validation Loss: 0.7144\n",
      "Epoch [185/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [186/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [187/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [188/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [189/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [190/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [191/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [192/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [193/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [194/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [195/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [196/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [197/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [198/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [199/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [200/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [201/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [202/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [203/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [204/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [205/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [206/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [207/100], Training Loss: 0.6068, Validation Loss: 0.7144\n",
      "Epoch [208/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [209/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [210/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [211/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [212/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [213/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [214/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [215/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [216/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [217/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [218/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [219/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [220/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [221/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [222/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [223/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [224/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [225/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [226/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [227/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [228/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [229/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [230/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [231/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [232/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [233/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [234/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [235/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [236/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [237/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [238/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [239/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [240/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [241/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [242/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [243/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [244/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [245/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [246/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [247/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [248/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [249/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [250/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [251/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [252/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [253/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [254/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [255/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [256/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [257/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [258/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [259/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [260/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [261/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [262/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [263/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [264/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [265/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [266/100], Training Loss: 0.6069, Validation Loss: 0.7144\n",
      "Epoch [267/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [268/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [269/100], Training Loss: 0.6070, Validation Loss: 0.7146\n",
      "Epoch [270/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [271/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [272/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [273/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [274/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [275/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [276/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [277/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [278/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [279/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [280/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [281/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [282/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [283/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [284/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [285/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [286/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [287/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [288/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [289/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [290/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [291/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [292/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [293/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [294/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [295/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [296/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [297/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [298/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [299/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [300/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [301/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [302/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [303/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [304/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [305/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [306/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [307/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [308/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [309/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [310/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [311/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [312/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [313/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [314/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [315/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [316/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [317/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [318/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [319/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [320/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [321/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [322/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [323/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [324/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [325/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [326/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [327/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [328/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [329/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [330/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [331/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [332/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [333/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [334/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [335/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [336/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [337/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [338/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [339/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [340/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [341/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [342/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [343/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [344/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [345/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [346/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [347/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [348/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [349/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [350/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [351/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [352/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [353/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [354/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [355/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [356/100], Training Loss: 0.6068, Validation Loss: 0.7145\n",
      "Epoch [357/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [358/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [359/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [360/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [361/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [362/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [363/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [364/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [365/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [366/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [367/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [368/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [369/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [370/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [371/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [372/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [373/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [374/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [375/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [376/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [377/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [378/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [379/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [380/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [381/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [382/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [383/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [384/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [385/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [386/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [387/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [388/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [389/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [390/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [391/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [392/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [393/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [394/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [395/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [396/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [397/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [398/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [399/100], Training Loss: 0.6067, Validation Loss: 0.7144\n",
      "Epoch [400/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [401/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [402/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [403/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [404/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [405/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [406/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [407/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [408/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [409/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [410/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [411/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [412/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [413/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [414/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [415/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [416/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [417/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [418/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [419/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [420/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [421/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [422/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [423/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [424/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [425/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [426/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [427/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [428/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [429/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [430/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [431/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [432/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [433/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [434/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [435/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [436/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [437/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [438/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [439/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [440/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [441/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [442/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [443/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [444/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [445/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [446/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [447/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [448/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [449/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [450/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [451/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [452/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [453/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [454/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [455/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [456/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [457/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [458/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [459/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [460/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [461/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [462/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [463/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [464/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [465/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [466/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [467/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [468/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [469/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [470/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [471/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [472/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [473/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [474/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [475/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [476/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [477/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [478/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [479/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [480/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [481/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [482/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [483/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [484/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [485/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [486/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [487/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [488/100], Training Loss: 0.6066, Validation Loss: 0.7145\n",
      "Epoch [489/100], Training Loss: 0.6068, Validation Loss: 0.7144\n",
      "Epoch [490/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [491/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [492/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [493/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [494/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [495/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [496/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [497/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [498/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [499/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [500/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [501/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [502/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [503/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [504/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [505/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [506/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [507/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [508/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [509/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [510/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [511/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [512/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [513/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [514/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [515/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [516/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [517/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [518/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [519/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [520/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [521/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [522/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [523/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [524/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [525/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [526/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [527/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [528/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [529/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [530/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [531/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [532/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [533/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [534/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [535/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [536/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [537/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [538/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [539/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [540/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [541/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [542/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [543/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [544/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [545/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [546/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [547/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [548/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [549/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [550/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [551/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [552/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [553/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [554/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [555/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [556/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [557/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [558/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [559/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [560/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [561/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [562/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [563/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [564/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [565/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [566/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [567/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [568/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [569/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [570/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [571/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [572/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [573/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [574/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [575/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [576/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [577/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [578/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [579/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [580/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [581/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [582/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [583/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [584/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [585/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [586/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [587/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [588/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [589/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [590/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [591/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [592/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [593/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [594/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [595/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [596/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [597/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [598/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [599/100], Training Loss: 0.6067, Validation Loss: 0.7144\n",
      "Epoch [600/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [601/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [602/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [603/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [604/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [605/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [606/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [607/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [608/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [609/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [610/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [611/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [612/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [613/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [614/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [615/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [616/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [617/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [618/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [619/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [620/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [621/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [622/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [623/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [624/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [625/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [626/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [627/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [628/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [629/100], Training Loss: 0.6062, Validation Loss: 0.7144\n",
      "Epoch [630/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [631/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [632/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [633/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [634/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [635/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [636/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [637/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [638/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [639/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [640/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [641/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [642/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [643/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [644/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [645/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [646/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [647/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [648/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [649/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [650/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [651/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [652/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [653/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [654/100], Training Loss: 0.6065, Validation Loss: 0.7144\n",
      "Epoch [655/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [656/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [657/100], Training Loss: 0.6066, Validation Loss: 0.7144\n",
      "Epoch [658/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [659/100], Training Loss: 0.6065, Validation Loss: 0.7145\n",
      "Epoch [660/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [661/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [662/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [663/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [664/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [665/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [666/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [667/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [668/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [669/100], Training Loss: 0.6064, Validation Loss: 0.7145\n",
      "Epoch [670/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [671/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [672/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [673/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [674/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [675/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [676/100], Training Loss: 0.6063, Validation Loss: 0.7145\n",
      "Epoch [677/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [678/100], Training Loss: 0.6067, Validation Loss: 0.7145\n",
      "Epoch [679/100], Training Loss: 0.6062, Validation Loss: 0.7145\n",
      "Epoch [680/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [681/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [682/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [683/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [684/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [685/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [686/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [687/100], Training Loss: 0.6064, Validation Loss: 0.7144\n",
      "Epoch [688/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [689/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Epoch [690/100], Training Loss: 0.6063, Validation Loss: 0.7144\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store loss values\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "patience = 500\n",
    "best_loss = np.inf\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(10000):  # Assuming a maximum of 100 epochs\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Average training loss for this epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/10000], Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Plotting the training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.title('Training Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6b7f2fc-c5ec-48c6-a286-9f4ee5800f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.8689052e-12, 8.8817842e-14, 3.5527137e-11, ..., 5.1159077e-13,\n",
       "       9.6722630e-13, 1.2789769e-13], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3fab2-e9d7-4e45-87ae-cd366918393c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
